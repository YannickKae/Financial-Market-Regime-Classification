{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b45b578",
   "metadata": {},
   "source": [
    "# Market Regime Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6475af",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ae3392",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd252a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (confusion_matrix, roc_curve, roc_auc_score, accuracy_score, precision_score,\n",
    "                             recall_score, f1_score, classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06f7485",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb4b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data (replace 'your_data.csv' with your actual file)\n",
    "data = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Assuming the columns in the dataset are named 'feature_1', 'feature_2', 'momentum_returns', 'value_returns'\n",
    "# Create a new column 'target' to store the label (1 for momentum, 0 for value)\n",
    "data['target'] = np.where(data['momentum_returns'] > data['value_returns'], 1, 0)\n",
    "\n",
    "# Select the feature variables and target variable\n",
    "X = data[['feature_1', 'feature_2']]\n",
    "y = data['target']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fe645c",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59731373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest classifier and fit it on the training data\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels and probabilities for the testing set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "y_pred_proba = rf_classifier.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df98feb",
   "metadata": {},
   "source": [
    "### Classfifcation Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dbec72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Compute ROC curve and ROC AUC score\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Compute classification metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "error_rate = 1 - accuracy\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "true_positive_rate = recall\n",
    "false_positive_rate = fpr[1]\n",
    "specificity = 1 - false_positive_rate\n",
    "true_negative_rate = specificity\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "prevalence = np.sum(y_test) / len(y_test)\n",
    "\n",
    "# Print classification metrics\n",
    "print(\"Classification Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Error Rate: {error_rate:.2f}\")\n",
    "print(f\"True Positive Rate (Recall): {true_positive_rate:.2f}\")\n",
    "print(f\"True Negative Rate (Specificity): {true_negative_rate:.2f}\")\n",
    "print(f\"False Positive Rate: {false_positive_rate:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"Prevalence: {prevalence:.2f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.matshow(cm, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(x=j, y=i, s=cm[i, j], va='center', ha='center', fontsize=16)\n",
    "plt.xticks([0, 1], ['Value', 'Momentum'])\n",
    "plt.yticks([0, 1], ['Value', 'Momentum'])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad05ca1",
   "metadata": {},
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e3feb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the new time series\n",
    "binary_bet_returns = []\n",
    "prob_weighted_returns = []\n",
    "\n",
    "for i, (value_prob, momentum_prob) in enumerate(y_pred_proba):\n",
    "    # Binary bet\n",
    "    if momentum_prob > value_prob:\n",
    "        binary_bet_returns.append(data.loc[X_test.index[i], 'momentum_returns'])\n",
    "    else:\n",
    "        binary_bet_returns.append(data.loc[X_test.index[i], 'value_returns'])\n",
    "\n",
    "    # Probability weighted bet\n",
    "    prob_weighted_returns.append(value_prob * data.loc[X_test.index[i], 'value_returns'] + momentum_prob * data.loc[X_test.index[i], 'momentum_returns'])\n",
    "\n",
    "# Calculate cumulative returns for each time series\n",
    "cumulative_momentum_returns = np.cumprod(1 + data.loc[X_test.index, 'momentum_returns']) - 1\n",
    "cumulative_value_returns = np.cumprod(1 + data.loc[X_test.index, 'value_returns']) - 1\n",
    "cumulative_binary_bet_returns = np.cumprod(1 + np.array(binary_bet_returns)) - 1\n",
    "cumulative_prob_weighted_returns = np.cumprod(1 + np.array(prob_weighted_returns)) - 1\n",
    "\n",
    "# Plot the cumulative returns\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(cumulative_momentum_returns, label='Momentum')\n",
    "plt.plot(cumulative_value_returns, label='Value')\n",
    "plt.plot(cumulative_binary_bet_returns, label='Binary Bet')\n",
    "plt.plot(cumulative_prob_weighted_returns, label='Probability Weighted Bet')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cumulative Returns')\n",
    "plt.title('Comparison of Strategy Performance')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6386ae7",
   "metadata": {},
   "source": [
    "### Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529c70ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performance_metrics(returns):\n",
    "    # Calculate mean return\n",
    "    mean_return = np.mean(returns)\n",
    "    \n",
    "    # Calculate standard deviation\n",
    "    std_dev = np.std(returns)\n",
    "    \n",
    "    # Calculate Sharpe Ratio (assuming a risk-free rate of 0)\n",
    "    sharpe_ratio = mean_return / std_dev\n",
    "    \n",
    "    # Calculate Maximum Drawdown\n",
    "    cum_returns = np.cumprod(1 + returns) - 1\n",
    "    peak = np.argmax(np.maximum.accumulate(cum_returns) - cum_returns)\n",
    "    trough = np.argmax(cum_returns[:peak])\n",
    "    max_drawdown = cum_returns[trough] - cum_returns[peak]\n",
    "    \n",
    "    return mean_return, std_dev, sharpe_ratio, max_drawdown\n",
    "\n",
    "# Calculate performance metrics for each time series\n",
    "momentum_metrics = calculate_performance_metrics(data.loc[X_test.index, 'momentum_returns'])\n",
    "value_metrics = calculate_performance_metrics(data.loc[X_test.index, 'value_returns'])\n",
    "binary_bet_metrics = calculate_performance_metrics(binary_bet_returns)\n",
    "prob_weighted_metrics = calculate_performance_metrics(prob_weighted_returns)\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"Momentum Performance Metrics:\")\n",
    "print(f\"Mean Return: {momentum_metrics[0]:.4f} | Standard Deviation: {momentum_metrics[1]:.4f} | Sharpe Ratio: {momentum_metrics[2]:.4f} | Max Drawdown: {momentum_metrics[3]:.4f}\")\n",
    "print(\"Value Performance Metrics:\")\n",
    "print(f\"Mean Return: {value_metrics[0]:.4f} | Standard Deviation: {value_metrics[1]:.4f} | Sharpe Ratio: {value_metrics[2]:.4f} | Max Drawdown: {value_metrics[3]:.4f}\")\n",
    "print(\"Binary Bet Performance Metrics:\")\n",
    "print(f\"Mean Return: {binary_bet_metrics[0]:.4f} | Standard Deviation: {binary_bet_metrics[1]:.4f} | Sharpe Ratio: {binary_bet_metrics[2]:.4f} | Max Drawdown: {binary_bet_metrics[3]:.4f}\")\n",
    "print(\"Probability Weighted Bet Performance Metrics:\")\n",
    "print(f\"Mean Return: {prob_weighted_metrics[0]:.4f} | Standard Deviation: {prob_weighted_metrics[1]:.4f} | Sharpe Ratio: {prob_weighted_metrics[2]:.4f} | Max Drawdown: {prob_weighted_metrics[3]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
